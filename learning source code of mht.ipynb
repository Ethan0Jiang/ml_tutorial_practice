{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d4ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import string\n",
    "\n",
    "from keras import constraints\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras.engine.base_layer import Layer\n",
    "from keras.layers import einsum_dense\n",
    "from keras.utils import tf_utils\n",
    "import numpy as np\n",
    "import tensorflow.compat.v2 as tf\n",
    "\n",
    "from tensorflow.python.platform import tf_logging as logging\n",
    "from tensorflow.python.util.tf_export import keras_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d53153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_proj_equation(free_dims, bound_dims, output_dims):\n",
    "    \"\"\"Builds an einsum equation for projections inside multi-head attention.\"\"\"\n",
    "    input_str = \"\"\n",
    "    kernel_str = \"\"\n",
    "    output_str = \"\"\n",
    "    bias_axes = \"\"\n",
    "    letter_offset = 0\n",
    "    for i in range(free_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        output_str += char\n",
    "\n",
    "    letter_offset += free_dims\n",
    "    for i in range(bound_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        input_str += char\n",
    "        kernel_str += char\n",
    "\n",
    "    letter_offset += bound_dims\n",
    "    for i in range(output_dims):\n",
    "        char = _CHR_IDX[i + letter_offset]\n",
    "        kernel_str += char\n",
    "        output_str += char\n",
    "        bias_axes += char\n",
    "    equation = \"%s,%s->%s\" % (input_str, kernel_str, output_str)\n",
    "    print(\"equation: \", equation)\n",
    "    print(\"bias_axes: \", bias_axes)\n",
    "    print(\"len(output_str) :\", len(output_str))\n",
    "    return equation, bias_axes, len(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de6978e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = tf.random.uniform(shape=[5, 100, 256])\n",
    "_query_shape = tf.TensorShape(query.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "932147ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "free_dims =_query_shape.rank - 1\n",
    "print(free_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d151bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_CHR_IDX = string.ascii_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b355b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equation:  abc,cde->abde\n",
      "bias_axes:  de\n",
      "len(output_str) : 4\n"
     ]
    }
   ],
   "source": [
    "einsum_equation, bias_axes, output_rank = _build_proj_equation(\n",
    "    free_dims, bound_dims=1, output_dims=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a2d1d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_attention_equation(rank, attn_axes):\n",
    "    \"\"\"Builds einsum equations for the attention computation.\n",
    "    Query, key, value inputs after projection are expected to have the shape as:\n",
    "    `(bs, <non-attention dims>, <attention dims>, num_heads, channels)`.\n",
    "    `bs` and `<non-attention dims>` are treated as `<batch dims>`.\n",
    "    The attention operations can be generalized:\n",
    "    (1) Query-key dot product:\n",
    "    `(<batch dims>, <query attention dims>, num_heads, channels), (<batch dims>,\n",
    "    <key attention dims>, num_heads, channels) -> (<batch dims>,\n",
    "    num_heads, <query attention dims>, <key attention dims>)`\n",
    "    (2) Combination:\n",
    "    `(<batch dims>, num_heads, <query attention dims>, <key attention dims>),\n",
    "    (<batch dims>, <value attention dims>, num_heads, channels) -> (<batch dims>,\n",
    "    <query attention dims>, num_heads, channels)`\n",
    "    Args:\n",
    "    rank: Rank of query, key, value tensors.\n",
    "    attn_axes: List/tuple of axes, `[-1, rank)`,\n",
    "      that attention will be applied to.\n",
    "    Returns:\n",
    "    Einsum equations.\n",
    "    \"\"\"\n",
    "    target_notation = _CHR_IDX[:rank]\n",
    "    # `batch_dims` includes the head dim.\n",
    "    batch_dims = tuple(np.delete(range(rank), attn_axes + (rank - 1,)))\n",
    "    letter_offset = rank\n",
    "    source_notation = \"\"\n",
    "    for i in range(rank):\n",
    "        if i in batch_dims or i == rank - 1:\n",
    "          source_notation += target_notation[i]\n",
    "        else:\n",
    "          source_notation += _CHR_IDX[letter_offset]\n",
    "          letter_offset += 1\n",
    "\n",
    "    product_notation = \"\".join([target_notation[i] for i in batch_dims] +\n",
    "                             [target_notation[i] for i in attn_axes] +\n",
    "                             [source_notation[i] for i in attn_axes])\n",
    "    dot_product_equation = \"%s,%s->%s\" % (source_notation, target_notation,\n",
    "                                        product_notation)\n",
    "    attn_scores_rank = len(product_notation)\n",
    "    combine_equation = \"%s,%s->%s\" % (product_notation, source_notation,\n",
    "                                    target_notation)\n",
    "    \n",
    "    print(\"dot_product_equation: \", dot_product_equation)\n",
    "    print(\"combine_equation: \", combine_equation)\n",
    "    print(\"len(attn_scores_rank) :\", len(attn_scores_rank))\n",
    "    \n",
    "    return dot_product_equation, combine_equation, attn_scores_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64c1b81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "_attention_axes = tuple(range(1, 4 - 2))\n",
    "print(_attention_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463687d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Batch,seq,dim), (dim,num_head,head_size) -> (Batch,seq,n_head,head_size)\n",
    "# (Batch,seq,dim), (dim,num_head,head_size) -> (Batch,seq,n_head,head_size)\n",
    "# (Batch,seq,dim), (dim,num_head,head_size_v) -> (Batch,seq,n_head,head_size_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f36e02b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('aecd,abcd->acbe', 'acbe,aecd->abcd', 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_build_attention_equation(4, _attention_axes) \n",
    "# self._dot_product_equation, self._combine_equation, attn_scores_rank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49b67ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_axes = tuple(range(4 - len(_attention_axes), 4))\n",
    "norm_axes # softmax on axis seq_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8200fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = tf.multiply(query, 1.0 / math.sqrt(float(self._key_dim)))\n",
    "attention_scores = tf.einsum(self._dot_product_equation, key, query) #:\n",
    "# 'aecd,abcd->acbe' (Batch,Seq_key,num_head,head_size), (Batch,Seq_query,num_head,head_size)\n",
    "# -> (Batch,num_head,seq_query,seq_key) Weight matrix\n",
    "\n",
    "attention_scores = self._masked_softmax(attention_scores, attention_mask) \n",
    "\n",
    "attention_output = tf.einsum(self._combine_equation, attention_scores, value) #:\n",
    "# 'acbe,aecd->abcd' (Batch,num_head,seq_query,seq_key), (Batch,Seq_value,num_head,head_size_v)\n",
    "# -> (Batch,Seq_query,num_head,head_size_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325ca394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Batch,Seq_query,num_head,head_size_v), (Batch,num_head,head_size_v,dim) -> (Batch,seq,dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
